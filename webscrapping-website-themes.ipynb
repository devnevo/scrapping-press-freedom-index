{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping World Press Freedom Index Report. \n",
    "\n",
    "### Source: [Reporters without Borders](https://rsf.org/en/ranking_table)\n",
    "\n",
    "![image.png](https://i.imgur.com/p3n7MZ9.png)\n",
    "\n",
    "[Web Scrapping](https://en.wikipedia.org/wiki/Web_scraping) is an automated process of gatering data from a server. It is usually accomplished by writing an automated program that queries a webserver, requests data(usually HTML) and parses the data to extract the required information. There are a number of ways to acheive this, but we are going to use requests, Beautiful Soup and pandas libraries.\n",
    "\n",
    "The [Press Freedom Index](https://rsf.org/en/ranking) is an annual ranking of countries compiled and published by [Reporters Without Borders](https://rsf.org/en) since 2002 based upon the organisation's own assessment of the countries' press freedom records in the previous year. The Index ranks 180 countries and regions according to the level of freedom available to journalists. Reporters Without Borders is an international non-profit and non-governmental organization with the stated aim of safeguarding the right to freedom of information. It describes its advocacy as founded on the belief that everyone requires access to the news and information, in line with Article 19 of the Universal Declaration of Human Rights that recognizes the right to receive and share information regardless of frontiers, along with other international rights charters. RSF has consultative status at the United Nations, UNESCO, the Council of Europe, and the International Organisation of the Francophonie.\n",
    "\n",
    "#### Outline:\n",
    "* Using the `requests` library, Fetch the HTML data of the `https://rsf.org/en/ranking` website.\n",
    "* Parse the DOM tree of the HTML page using the Beautiful Soup() method provided by the `Beautiful Soup` library.\n",
    "* Identify the patterns and attributes like ids, classes and use them to fetch the elements containing the required data.\n",
    "* Compile the extracted information into data using Python lists and libraries.\n",
    "* Save the extracted Information into a csv file.\n",
    "\n",
    "In the end, here's what the csv will look:\n",
    "```\n",
    "Rank,Country,Abuse Score,Global Score,Detail Url,Situation Score,Journalist Killings,Citizen Journalist Killings,Media Assistants Killings,2020,2019,2018,2017,2016,2015,2014,2013\n",
    "1,Norway,0,6.72,https://rsf.org//en/norway,6.72,0,0,0,1,1,1,1,3,2,3,3\n",
    "2,Finland,0,6.99,https://rsf.org//en/finland,6.99,0,0,0,2,2,4,3,1,1,1,1\n",
    "...\n",
    "```\n",
    "\n",
    "Libraries Used: `requests, Beautiful Soup, Pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `Run` button at the top of the page to execute the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(project=\"webscrapping-pressfreedom-report\", git_commit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Using requests library to get the HTML page\n",
    "\n",
    "- We use the get method provided by the requests library to fetch the page from the webserver.\n",
    "- we pass in the site url to the get method and check the status code to ensure whether the call is a success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_url = 'https://rsf.org/en/ranking_table'\n",
    "base_url = 'https://rsf.org/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the `requests` library from pip and importing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_response = requests.get(site_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the status code of the request to check if the page is loaded successfully. Status codes ranging between `200-299` are considered successful responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = site_response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('press_freedom_rankings.html', 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the HTML response text to a html file and saving it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Using Beautiful Soup library to extract data from the webpage.\n",
    "\n",
    "- We create a parse tree for HTML page(response_text) using the `BeautifulSoup()` method that can be used to extract data from HTML.\n",
    "- We can now traverse the DOM tree of the HTML page to extract the required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the `Beautiful Soup` library from pip and importing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_web_content = BeautifulSoup(response_text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup method creates a parse tree for the HTML page so that we can get the required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbody_element = parsed_web_content.find('tbody')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the HTML element with `tag: 'tbody'` \n",
    "![image1](https://i.imgur.com/cUYadZu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_entity_rows = tbody_element.find_all('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `tr` tag corresponds to HTML content of each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entity_rows[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`all_entity_rows` is a list of snippets containing the data of a row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Consider the example of scrapping data of the country `Norway`. \n",
    "\n",
    "- Here we try to fetch the data of the country Norway to understand the stratergy behind fetching the required data by identifying the patterns(like classes, ids and other attributes) in the HTML code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_index = 0\n",
    "data_cells = all_entity_rows[example_index].find_all('td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retained_cells = data_cells[2: len(data_cells) - 2] #Slicing the array to retain the required cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to format and get the required values in a row\n",
    "def get_row_values(row):\n",
    "    return_array = []\n",
    "    for index, cell in enumerate(row):\n",
    "        if index == 0:\n",
    "            country = cell.find('a')\n",
    "            return_array.append(country.text.strip())\n",
    "            return_array.append(base_url+country['href'])\n",
    "        else:\n",
    "            return_array.append(cell.text.strip())\n",
    "    return return_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calling the `get_row_values()` method by passing the row of a parsed table which contains all the data of the country Norway.\n",
    "* Assigning the values of `Country, Details Url, Abuse Score, Underlying Situation Score, Global Score` to respective variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country, details_url, abuse_score, situation_score, global_score = get_row_values(retained_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Country: ', country)\n",
    "print('Details Url: ', details_url)\n",
    "print('Abuse Score: ', abuse_score)\n",
    "print('Underlying Situation Score: ', situation_score)\n",
    "print('Global Score: ', global_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the Country specific page(`In our case Norway`) to parse `Rankings of the country since 2013, Journalist Deaths, Citizen Journalist Deaths and Media Assistants Deaths` Data using Details Url\n",
    "\n",
    "![image2](https://i.imgur.com/YMIjkvT.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a parsed tree of a html page\n",
    "def get_html_page(page_url):\n",
    "    html_page = requests.get(page_url)\n",
    "    if html_page.status_code == 200:\n",
    "        response_text = html_page.text\n",
    "        parsed_web_content = BeautifulSoup(response_text, 'html.parser')\n",
    "        return parsed_web_content\n",
    "    else:\n",
    "        raise Exception('Failed to load the page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content_details = get_html_page(details_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to format and get the details from an individual country page \n",
    "def get_other_info_from_each_url(url):\n",
    "    elements = page_content_details.find_all('div', class_='js-animated-number')\n",
    "    def return_values(value):\n",
    "        return value.text.strip()\n",
    "    formatted_elems = map(return_values, elements)\n",
    "    return list(formatted_elems)\n",
    "\n",
    "journalist_deaths, citizen_journalist_deaths, media_assist_deaths = get_other_info_from_each_url(details_url)\n",
    "rank_element = page_content_details.find('div', class_='white-b__ranking-score')\n",
    "rank = rank_element.text.strip()\n",
    "\n",
    "print('Journalist Killings:', journalist_deaths)\n",
    "print('Citizen Journalist Killings:', citizen_journalist_deaths)\n",
    "print('Media Assistant Killings:', media_assist_deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get previous years rankings for the country\n",
    "rankings = {}\n",
    "def get_previous_year_rankings():\n",
    "    table_body_elements = page_content_details.find_all('tbody')\n",
    "    for element in table_body_elements:\n",
    "        rows = element.find_all('tr')[1: len(element)]\n",
    "        for cell in rows:\n",
    "            cell_items = cell.find_all('td')\n",
    "            year = str(cell_items[0].text.strip())\n",
    "            rank = cell_items[1].text.split('/')[0].strip()\n",
    "            if not year in rankings.keys():\n",
    "                rankings[year] = []\n",
    "            rankings[year].append(rank.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the `get_previous_year_rankings()` method to fetch the previous years ranking of the country and adding them to the rankings dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_previous_year_rankings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Final data scrapped for the country `Norway`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current Rank: ', rank)\n",
    "print('Country: ', country)\n",
    "print('Details Url: ', details_url)\n",
    "print('Abuse Score: ', abuse_score)\n",
    "print('Underlying Situation Score: ', situation_score)\n",
    "print('Global Score: ', global_score)\n",
    "print('Journalist Killings:', journalist_deaths)\n",
    "print('Citizen Journalist Killings:', citizen_journalist_deaths)\n",
    "print('Media Assistant Killings:', media_assist_deaths)\n",
    "for year in rankings:\n",
    "    print('Ranking for the year {year}: {rank}'.format(year=year, rank=rankings[year][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Scrapping the Data for Multiple Countries.\n",
    "\n",
    "* Here we try to scrape the data of the first 30 countries.\n",
    "* We use the methods above and run them in a for loop specifying the number of countries we want to fetch the data for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "countries = []\n",
    "abuse_scores = []\n",
    "global_scores = []\n",
    "details_urls = []\n",
    "situation_scores = []\n",
    "list_journalist_deaths = []\n",
    "list_citizen_journalist_deaths = []\n",
    "list_media_assist_deaths = []\n",
    "rankings = {}\n",
    "\n",
    "for i in range(1, 31):\n",
    "    data_cells = all_entity_rows[i].find_all('td') # Getting the rows, each row corresponding to each country\n",
    "    retained_cells = data_cells[2: len(data_cells) - 2]  # retaining the required data cells\n",
    "    country, details_url, abuse_score, situation_score, global_score = get_row_values(retained_cells)\n",
    "    print('Scraping data for the country: ', country)\n",
    "    page_content_details = get_html_page(details_url) # Fetching the individual country page to get additional data\n",
    "    journalist_deaths, citizen_journalist_deaths, media_assist_deaths = get_other_info_from_each_url(details_url)\n",
    "    rank_element = page_content_details.find('div', class_='white-b__ranking-score')\n",
    "    rank = rank_element.text.strip()\n",
    "    get_previous_year_rankings()\n",
    "    \n",
    "    # appending the values to the output arrays\n",
    "    ranks.append(rank)\n",
    "    countries.append(country)\n",
    "    details_urls.append(details_url)\n",
    "    abuse_scores.append(abuse_score)\n",
    "    global_scores.append(global_score)\n",
    "    situation_scores.append(situation_score)\n",
    "    list_journalist_deaths.append(journalist_deaths)\n",
    "    list_citizen_journalist_deaths.append(citizen_journalist_deaths)\n",
    "    list_media_assist_deaths.append(media_assist_deaths)\n",
    "    \n",
    "scrappedData_final = {\n",
    "    'Rank': ranks,\n",
    "    'Country': countries,\n",
    "    'Abuse Score': abuse_scores,\n",
    "    'Global Score': global_scores,\n",
    "    'Detail Url': details_urls,\n",
    "    'Situation Score': situation_scores,\n",
    "    'Journalist Deaths': journalist_deaths,\n",
    "    'Citizen Journalist Deaths': citizen_journalist_deaths,\n",
    "    'Media Assistants Deaths': media_assist_deaths\n",
    "}\n",
    "\n",
    "scrappedData_final.update(rankings); # adding year wise rankings to the data dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Using Pandas to convert the dictionary of scrapped data to a DataFrame\n",
    "\n",
    "The dictionary of values is converted to a DataFrame using `pd.DataFrame` method provided by Pandas.\n",
    "\n",
    "* A Pandas DataFrame is a 2 dimensional data structure, like a 2 dimensional array, or a table with rows and columns.\n",
    "* It offers data structures and operations for manipulating numerical tables and time series.\n",
    "* It is a primary datastructure in pandas.\n",
    "* Here each list of values in the dict themes_information is converted into a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the `pandas` library from pip and importing it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_total_Info = pd.DataFrame(scrappedData_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_total_Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_total_Info.to_csv('press-freedom-index.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `pd.to_csv` method provided by pandas to save the data frame to a csv file(`press-freedom-index.csv`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Putting Everything Together\n",
    "\n",
    "### Scraping the data for all the 180 countries and save it to a csv file(`press-freedom-index.csv`).\n",
    "\n",
    "#### Data we are scrapping:\n",
    "* Name of the Country\n",
    "* Current rank\n",
    "* Details page url.\n",
    "* Abuse Score\n",
    "* Underlying situation score\n",
    "* Global score\n",
    "* Previous rankings (2013-2020)\n",
    "* Journalists killed (2021)\n",
    "* Citizen Journalists killed (2021)\n",
    "* Media assistants killed (2021)\n",
    "\n",
    "#### Stratergy:\n",
    "\n",
    "* Using the [base url](https://rsf.org/en/ranking_table), we fetch the HTML page using the requests library.\n",
    "* The HTML code is converted to a parsing tree using BeautifulSoup library.\n",
    "* Using the methods provided by the BeautifulSoup library, we parse certain HTML snippets with identifiers such as class names, ids, attributes etc.\n",
    "* In case of the above project, we parsed a tbody HTML snippet with `<tr>`(rows) corresponding to the data(`<td>`) each country.\n",
    "* After attributes fetched here are `Name, Details page url, Abuse score, Underlying situation score and global score`.\n",
    "* Now, we loop through the urls list and fetch the HTML pages of each individual country using the same methodology mentioned above.\n",
    "* Here, we fetch the data of `Previous rankings(2013-2020), Current ranking, Journalists killed, Citizen Journalists killed, Media assistants killed for the year 2021`.\n",
    "* The lists of data is put together into a dict(`scrappedData_final`) with keys pertaining to the name of the columns.\n",
    "* Using pandas library, we now convert the data into a DataFrame which provides with various method to manipute and analyze the data.\n",
    "* Using the method `to_csv` provided by pandas, we now save the Dataframe to a csv file(`press-freedom-index.csv`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #packages that is used to download the content from web\n",
    "import os # package used for file process\n",
    "from bs4 import BeautifulSoup #a Python library for pulling data out of HTML and XML files\n",
    "import pandas as pd # the omnipresent of all python to work with dataframes\n",
    "import numpy as np\n",
    "import jovian\n",
    "from IPython.core.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_url = 'https://rsf.org/en/ranking_table'\n",
    "base_url = 'https://rsf.org/'\n",
    "\n",
    "list_countries = []\n",
    "list_ranking = []\n",
    "list_details_urls = []\n",
    "list_abuse_scores = [] \n",
    "list_global_scores = []\n",
    "list_situation_scores = []\n",
    "list_journalist_killings = []\n",
    "list_citizen_journalist_killings = []\n",
    "list_media_assistants_killings = []\n",
    "rankings = {}\n",
    "\n",
    "#Function to fetch individual cell values\n",
    "def fetch_data_cell_values(row):\n",
    "    #gathering the data of each data cell and appending it to a list\n",
    "    cell = []\n",
    "    for index, i in enumerate(row):\n",
    "        if index == 0:\n",
    "            country = i.find('a')\n",
    "            cell.append(country.text.strip()) #country name \n",
    "            cell.append(base_url + country['href']) # details url of the country\n",
    "        else:\n",
    "            cell.append(i.text.strip()) #other values \n",
    "    return cell\n",
    "    \n",
    "def get_list_of_rows(parsed_web_content):\n",
    "    # parsing the tbody snippet which has all the data(tr tags)\n",
    "    tbody_element = parsed_web_content.find('tbody')\n",
    "    # parsing the rows inside the tbody tag(each row has multiple data cells, each data cell corresponds to a certain attribute)\n",
    "    data_rows = tbody_element.find_all('tr')\n",
    "    return data_rows\n",
    "\n",
    "def get_list_of_data_cells(row):\n",
    "    #parsing data cell for each row\n",
    "    data_cells = row.find_all('td')\n",
    "    #slicing the array to retain the required cells\n",
    "    required_data_cells = data_cells[2: len(data_cells) - 2] #Slicing the array to retain the required cells\n",
    "    return required_data_cells\n",
    "\n",
    "#Function to download and parse the html of the page\n",
    "def get_parsed_html_page(page_url):\n",
    "    #using requests to get the html content\n",
    "    html_page = requests.get(page_url)\n",
    "    #checking for the status of the call\n",
    "    if html_page.status_code == 200:\n",
    "        response_text = html_page.text\n",
    "        #using beautiful soup library fetching the parsing tree of the html page\n",
    "        parsed_web_content = BeautifulSoup(response_text, 'html.parser')\n",
    "        return parsed_web_content\n",
    "    else:\n",
    "        raise Exception('Failed to load the page')\n",
    "\n",
    "def get_country_specific_data(url):\n",
    "    #Fetching the html content of the single country page\n",
    "    parsed_html_page = get_parsed_html_page(url)\n",
    "    #getting the archived rankings of the country ( 2013-2020) and appending them to a dict of lists\n",
    "    get_previous_rankings(parsed_html_page)\n",
    "    #Gathering other info from the page\n",
    "    get_other_info_from_each_url(parsed_html_page)\n",
    "\n",
    "def get_other_info_from_each_url(page_content):\n",
    "    #parsing div with class 'js-animated-number' which returns 3 elements each with a value\n",
    "    elements = page_content.find_all('div', class_='js-animated-number')\n",
    "    def return_values(value):\n",
    "        return value.text.strip()\n",
    "    formatted_elems = map(return_values, elements)\n",
    "    #parsing the element with rank value and appending it the list\n",
    "    rank_element = page_content.find('div', class_='white-b__ranking-score')\n",
    "    list_ranking.append(rank_element.text.strip())\n",
    "    #Assigning the values to respective variables and appending them to the arrays\n",
    "    journalist_deaths, citizen_journalist_deaths, media_assist_deaths = formatted_elems\n",
    "    list_journalist_killings.append(journalist_deaths)\n",
    "    list_citizen_journalist_killings.append(citizen_journalist_deaths)\n",
    "    list_media_assistants_killings.append(media_assist_deaths)\n",
    "\n",
    "def get_previous_rankings(page_content):\n",
    "    #parsing 2 tbody snippets\n",
    "    table_body_elements = page_content.find_all('tbody')\n",
    "    expected_array_cells = [str(num) for num in range(2013, 2021)]\n",
    "    row_index = []\n",
    "    for element in table_body_elements:\n",
    "        #getting tr(row) which had year and rank data in each cell\n",
    "        rows = element.find_all('tr')[1: len(element)]\n",
    "        for cell in rows:\n",
    "            #obtaining data cell with each containing year and rank \n",
    "            cell_items = cell.find_all('td')\n",
    "            year = str(cell_items[0].text.strip()) #year value\n",
    "            rank = cell_items[1].text.split('/')[0].strip() #rank value\n",
    "            row_index.append(year)\n",
    "            if year not in rankings.keys():\n",
    "                rankings[year] = []\n",
    "            rankings[year].append(rank)\n",
    "    # Filling the empty positions with none value\n",
    "    for year in expected_array_cells:\n",
    "        if year not in row_index:\n",
    "            if year not in rankings.keys():\n",
    "                rankings[year] = []\n",
    "            rankings[year].append(np.nan)\n",
    "            \n",
    "def arrange_and_display_dataframe():\n",
    "    #Putting all the data scrapped in to a dict\n",
    "    scrappedData_final = {\n",
    "        'Rank': list_ranking,\n",
    "        'Country': list_countries,\n",
    "        'Abuse Score': list_abuse_scores,\n",
    "        'Global Score': list_global_scores,\n",
    "        'Detail Url': list_details_urls,\n",
    "        'Situation Score': list_situation_scores,\n",
    "        'Journalist Killings': list_journalist_killings,\n",
    "        'Citizen Journalist Killings': list_citizen_journalist_killings,\n",
    "        'Media Assistants Killings': list_media_assistants_killings\n",
    "    }\n",
    "\n",
    "    #merging the dict with few other parameters \n",
    "    scrappedData_final.update(rankings)\n",
    "\n",
    "    #Creating a data from the dict using pandas library \n",
    "    ranking_total_Info = pd.DataFrame(scrappedData_final)\n",
    "    \n",
    "    return ranking_total_Info;\n",
    " \n",
    "            \n",
    "def scrape_press_freedom_index():\n",
    "    #parsing the html data\n",
    "    parsed_html_page = get_parsed_html_page(site_url)\n",
    "    #getting the rows(each row consists of the data of a certain country)\n",
    "    data_rows = get_list_of_rows(parsed_html_page)\n",
    "    for index, row in enumerate(data_rows):\n",
    "        #getting the data cells/column(each cell consists of the data of a certain parameter of the report)\n",
    "        data_cells = get_list_of_data_cells(row)\n",
    "                                           \n",
    "        #appending the values to respective arrays\n",
    "        country, details_url, abuse_score, situation_score, global_score = fetch_data_cell_values(data_cells)\n",
    "        list_countries.append(country)\n",
    "        list_details_urls.append(details_url)\n",
    "        list_abuse_scores.append(abuse_score)\n",
    "        list_situation_scores.append(situation_score)\n",
    "        list_global_scores.append(global_score)\n",
    "        print('Fetching details for {country} '.format(country=country))\n",
    "                                           \n",
    "        #Fetching the html page of each country and scrapping the required there                                 \n",
    "        get_country_specific_data(details_url)\n",
    "\n",
    "    #Arranging the scrapped data and creating it into a dataframe and save it to a csv file\n",
    "    data_frame = arrange_and_display_dataframe()\n",
    "    \n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the method `scrape_press_freedom_index()` to scrape the data of press freedom report for 180 countries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = scrape_press_freedom_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the dataframe to a csv file with name 'press-freedom-index.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv('press-freedom-index.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 200, 'display.max_columns', 20):\n",
    "    display(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.Summary\n",
    "\n",
    "In this project, we are trying the scrape the data of `Press freedom ranking 2021`. It is Published every year since 2002 by `Reporters Without Borders (RSF)`, the World Press Freedom Index is an important advocacy tool based on the principle of emulation between states. The Index ranks `180 countries and regions` according to the level of freedom available to journalists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.References\n",
    "\n",
    "* Requests library source: [Requests Documentation](https://docs.python-requests.org/en/master/)\n",
    "* Beautiful Soup libray source: [Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* Pandas library source: [Documentation](https://pandas.pydata.org/docs/)\n",
    "* Python programming source: [Doumentation](https://www.python.org/doc/)  \n",
    "* Course lecture source: [lecture](https://jovian.ai/learn/zero-to-data-analyst-bootcamp/lesson/web-scraping-and-rest-apis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching data from a REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = requests.get('https://api.covidtracking.com/v1/us/daily.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Status code\n",
    "json_data.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_text = json_data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using loads method to convert the string to a dict\n",
    "formatted_data = json.loads(HTML_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data[:5] #json converted to a dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jovian.commit(project=\"webscrapping-pressfreedom-report\", files=['press-freedom-index.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}